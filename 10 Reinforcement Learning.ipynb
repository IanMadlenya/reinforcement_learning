{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Reinforcement Learning Example\n",
    "\n",
    "This notebook will guide you through implementing a gradient policy algorithm in neon to train an agent to play Pong. You will learn\n",
    "- Using the Autodiff interface to perform automatic differentiation and obtain gradients given an op-tree\n",
    "- Implement a neural network that adjusts its parameters and policies with gradients to get better rewards directly from experiences. This model samples the expected policy from a distribution. \n",
    "  \n",
    "\n",
    "## Preamble\n",
    "\n",
    "Reinforcement Learning trains an agent to take actions in an unknown environment based on experiences. Unlike supervised learning, actual labels are not always available so RL algorithms explores the environment by interacting with actions and receives corresponding feedbacks as positive or negative rewards. \n",
    "\n",
    "In this tutorial, we will train an agent to play Pong with Stochastic Policy Gradients using [OpenAI Gym](https://gym.openai.com/) as the environment and [neon](https://neon.nervanasys.com/index.html/) as our deep learning framework. \n",
    "\n",
    "![Agent interacting with its environment](data/pong_rl_demo.gif)\n",
    "\n",
    "As described in [Andrei Karpathy's blog](http://karpathy.github.io/2016/05/31/rl/) Policy Gradients has shown to perform better than DQN algorithms by most people including DQN Authors (https://www.youtube.com/watch?v=M8RfOCYIL8k), so we will use it to \n",
    "\n",
    "The underlying model is a 2-layer fully connected neural network that receives image frames as inputs, explores rewards given stochastic actions (probability of moving a paddle UP/DOWN) in an unknown environment, and updates weights by maximizing a temporal cumulative reward function modulated by an advantage as depicted below.  \n",
    "\n",
    "![RL algorithm](data/pong_architecture.jpg)\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "This example works with Python 2.7. \n",
    "\n",
    "Your environment needs to have the following packages installed:\n",
    "- neon v1.9.0\n",
    "- OpenAI Gym for initializing Atari game environments\n",
    "- numpy for random initialization and aritmetic operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "\n",
    "We will guide you through implementing a policy function parameterized by a neural network. We first import all the needed ingredients: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from neon.backends import gen_backend\n",
    "from neon.backends import Autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also set up the backend and define the class containing our network. This consists of two layers 'W1' and 'W2' with values randomly initialized, but trainable with gradient updates. \n",
    "\n",
    "We have included functions implementing the forward and backward propagation steps. The forward step generates a policy given an a visual representation of the environment stored in variable x. The back propagation function updates the layers of the network modulating the loss function values with discounted rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "be = gen_backend('cpu', batch_size=128)\n",
    "class Network:\n",
    "    def __init__(self, D = 80*80, H = 200, gamma = 0.99):\n",
    "        '''\n",
    "        D: number image pixels\n",
    "        H: number of hidden units in first layer of neural network\n",
    "        gamma: discount factor\n",
    "        '''\n",
    "        self.gamma = gamma\n",
    "        self.ll = {}\n",
    "        self.learning_rate = 0.001\n",
    "\n",
    "        self.ll['W1'] = be.array(np.random.randn(H,D) / np.sqrt(D)) #be.zeros((H,D))\n",
    "        self.ll['W2'] = be.array(np.random.randn(H,1) / np.sqrt(H)) #be.zeros((H,1))\n",
    "\n",
    "    # forward propagation\n",
    "    def policy_forward(self, x):\n",
    "        # map visual input to the first hidden layer of a neural network\n",
    "        # a larger number of units will increase the capacity of the network to learn different game states\n",
    "        # different local minima in this context represents different strategies giving the same game output\n",
    "        h = be.dot(self.ll['W1'],  be.array(x))\n",
    "        h = be.tanh(h)\n",
    "        logp = be.dot(h.transpose(), self.ll['W2'])\n",
    "\n",
    "        #probability of moving paddle up and hidden state\n",
    "        p = be.sig(logp)\n",
    "\n",
    "        #execute:\n",
    "        p_val = be.empty((1, 1))\n",
    "        h_val = be.empty((200, 1))\n",
    "        p_val[:] = p\n",
    "        h_val[:] = h\n",
    "\n",
    "        return p_val.get(), h_val.get(), p, h\n",
    "\n",
    "    # backward propagation\n",
    "    def policy_backward(self, losses_op, episode_losses, episode_rewards):\n",
    "        discounted_rewards = self.discount_rewards(episode_rewards)\n",
    "        # to reduce the variance of the gradient estimator and avoid potential vanishing problems\n",
    "        # when computing gradients: http://www.scholarpedia.org/article/Policy_gradient_methods\n",
    "        discounted_rewards -= np.mean(discounted_rewards)\n",
    "        discounted_rewards /= np.std(discounted_rewards)\n",
    "\n",
    "        episode_losses *= discounted_rewards  # to modulate gradients with discount factors\n",
    "\n",
    "        #compute partial derivatives using neon backend\n",
    "        for i in range(len(losses_op)):\n",
    "            ad = Autodiff(op_tree=losses_op[i] * be.array(discounted_rewards[i]), be=be, next_error=None)\n",
    "            # gradients\n",
    "            dW1, dW2 = ad.get_grad_asnumpyarray([self.ll['W1'], self.ll['W2']])\n",
    "            # execute:\n",
    "            dW1_val = be.empty((200, 6400))\n",
    "            dW2_val = be.empty((200, 1))\n",
    "            dW1_val[:] = dW1\n",
    "            dW2_val[:] = dW2\n",
    "            self.ll['W2'][:] = self.ll['W2'].get() + self.learning_rate * dW2_val.get()\n",
    "            self.ll['W1'][:] = self.ll['W1'].get() + self.learning_rate * dW1_val.get()\n",
    "        return\n",
    "\n",
    "    def sigmoid(self, x): \n",
    "        return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
    "\n",
    "    def get_loss(self, y, up_probability):\n",
    "        loss = y-up_probability\n",
    "        return loss\n",
    "    \n",
    "    # which actions lead to winning a game?: given this episodic environment, we assign rewards per time-step assuming\n",
    "    # there is a stationary distribution for a policy within the episode.\n",
    "    #   reward < 0, if agent missed the ball and hence lose the game\n",
    "    #   reward > 0, if agent won the game\n",
    "    #   reward == zero, any other state during the duration of a game\n",
    "    # the agent receives rewards generated by the game and implements discounted reward backwards with exponential\n",
    "    # moving average. More weight is given to earlier rewards. Reset to zero when game ends.\n",
    "    def discount_rewards(self, r):\n",
    "      discounted_r = np.zeros_like(r)\n",
    "      for t in reversed(range(0, r.size)):\n",
    "        # if reward at index t is nonzero, then there is a positive/negative reward. This also marks a game boundary\n",
    "        # for the sequence of actions produced by the agent\n",
    "        if r[t] != 0: running_add = 0 \n",
    "        # moving average given discount factor gamma, it assigns more weight on recent actions\n",
    "        discounted_r[t] = r[t] + discounted_r[t] * self.gamma\n",
    "      return discounted_r\n",
    "\n",
    "    # takes a single game frame as input and preprocesses before feeding into model\n",
    "    def prepro(self, I):\n",
    "      \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "      I = I[35:195] # crop\n",
    "      I = I[::2,::2,0]              # downsample by factor of 2\n",
    "      I[I == 144] = 0               # erase background (background type 1)\n",
    "      I[I == 109] = 0               # erase background (background type 2)\n",
    "      I[I != 0] = 1                 # everything else (paddles, ball) just set to 1\n",
    "      return I.astype(np.float).ravel() #flattens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Part of the magic behind reinforcement learning is related to the notion of optimizing objective functions under uncertain or non-differentiable scenarios considering a stochastic process. In our case, we sample the action from the probabilities returned by the last layer of a neural network and consider it as a temporal label. This logic is implemented in the following function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# stochastic process to choose an action (moving up) proportional to its\n",
    "# predicted probability. If such value is high, it's more\n",
    "# likely to sample an up action from this stochastic process.\n",
    "# Probablity of choosing the opposite action is (1-probability_up)\n",
    "#    action_ == 2, moving up\n",
    "#    action_ == 3, moving down\n",
    "def sample_action(up_probability):\n",
    "    stochastic_value = np.random.uniform()\n",
    "    action = 2 if stochastic_value < up_probability else 3\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Initialization of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "D = 80 * 80                 # number of pixels in input\n",
    "H = 200                     # number of hidden layer neurons\n",
    "# Pong environment\n",
    "env = gym.make(\"Pong-v0\")\n",
    "network = Network(D=D, H=H)\n",
    "\n",
    "# Each time step, the agent chooses an action, and the environment returns an observation \n",
    "# and a reward. The process gets started by calling reset, which returns an initial observation\n",
    "observation = env.reset()\n",
    "prev_x = None\n",
    "\n",
    "# hidden state, gradient ops, gradient values, rewards\n",
    "hs, losses_op, losses_values, rewards, h_values, x_values = [],[],[], [], [], []\n",
    "running_reward = None       # current reward\n",
    "reward_sum = 0.0            # sum rewards\n",
    "episode_number = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train!\n",
    "\n",
    "Our goal is to train an agent to win Pong against its opponent. An action consists of moving a paddle UP/DOWN, this eventually generates a positive reward (+1) if the trained agent wins a game or negative one (-1) if agent misses the ball.\n",
    "\n",
    "Before knowing the result of a game, the model gets a fake label via the stochastic process explained before. This is like tossing a coin to decide to accept the log probabilities of a neural network. An optimal set of actions will maximize the sum of rewards along the game. An import event is when the agent wins/losses a game. But what caused this outcome?. The algorithm decided to modulate the loss functions of the network with the positive or negative rewards obtained from the environment and assign more weight to earlier actions using a moving average scheme. This logic is implement in function policy_backward() of the Network class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    cur_x = network.prepro(observation)\n",
    "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "    prev_x = cur_x\n",
    "\n",
    "    up_probability, h_value, p, h = network.policy_forward(x)\n",
    "    action = sample_action(up_probability)\n",
    "\n",
    "    y = 1 if action == 2 else 0                 # assign a fake label, this decreases uncertainty and\n",
    "    loss_value = y - up_probability             # this is one of the beauties of Reinforcement Learning\n",
    "    losses_values.append(loss_value)\n",
    "    losses_op.append(y - p)\n",
    "    h_values.append(h_value)\n",
    "    env.render()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    rewards.append(reward)\n",
    "    reward_sum += reward\n",
    "    x_values.append(x)\n",
    "\n",
    "    if done:\n",
    "        episode_number +=1\n",
    "        episode_losses = np.vstack(losses_values)\n",
    "        episode_rewards = np.vstack(rewards)\n",
    "        losses_values, rewards, h_values = [], [], []\n",
    "\n",
    "        network.policy_backward(losses_op, episode_losses, episode_rewards)\n",
    "\n",
    "        losses_op = []\n",
    "        x_values = []\n",
    "\n",
    "        mean_loss = np.sum([x * x for x in episode_losses])\n",
    "        running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "\n",
    "        print('-----------------------------------------------')\n",
    "        print('Episode %d has finished, time to backpropagate.' % (episode_number - 1))\n",
    "        print('Total reward was %f Running_reward: %f Mean_loss: %f' % (reward_sum, running_reward, mean_loss))\n",
    "        print('-----------------------------------------------')\n",
    "\n",
    "        reward_sum = 0\n",
    "        observation = env.reset()  # reset env\n",
    "        prev_x = None\n",
    "\n",
    "    if reward != 0:  # Pong has either +1 or -1 reward exactly when game ends.\n",
    "        message = \"Episode %d: game finished. Reward: %f Loss: %f\\t\" % (episode_number, reward, loss_value)\n",
    "        if reward == -1:\n",
    "            message += \"\\x1b[0;31;40m  (RL loses)\\x1b[0m\"\n",
    "        else:\n",
    "            message += \"\\x1b[0;32;40m  (RL wins)\\x1b[0m\"\n",
    "        print(message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradients\n",
    "\n",
    "Consider the below visualization to get an intuition about how policy gradients behave in reinforcement learning. We map to 2D the weights 'W2' of the network before and after policy backpropagation takes place after 100 episodes. \n",
    "\n",
    "Left plot: The points in green correspond to observations associated to positive rewards and the ones in red are samples producing negative or zero rewards. Note how unbalanced is this problem as positive rewards are by definition sparse.   \n",
    "\n",
    "Right plot: The arrows represent the gradient of each sample towards the direction that increases its expected reward. Since the loss values of the network are weighted by their standardized  cumulative rewards in an episode, we expect the mean of a distribution to be influenced by the few gradients with positive rewards after parameter updates. Indeed, regions around green points seem to have small gradients indicating the preference of the network to keep those weight values. On the contrary, other regions often have large gradients which means back propagation algorithm notably updates their values in seek of better regions for maximizing the overall reward. After parameter update, the distribution will slowly move towards points with positive rewards. This is a slow process and the reason why reinforcement learning algorithms needs lots of training time. \n",
    "\n",
    "![RL algorithm](data/gradient_update.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other episodes.\n",
    "\n",
    "Episode 115:\n",
    "![RL algorithm](data/gradient_update-115.png)\n",
    "\n",
    "Episode 135:\n",
    "![RL algorithm](data/gradient_update-135.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
